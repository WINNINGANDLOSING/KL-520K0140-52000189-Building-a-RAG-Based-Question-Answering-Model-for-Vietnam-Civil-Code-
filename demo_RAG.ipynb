{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding model\n",
    "def get_embedding_model(model_name=\"/teamspace/studios/this_studio/bge-small-en-v1.5\"):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    return embed_model\n",
    "\n",
    "# generator model\n",
    "def get_llm(model_name=\"llama3-8b-8192\"):\n",
    "    llm = Groq(model=model_name, api_key=os.getenv(\"GROQ_API\"), temperature=0.8)\n",
    "    return llm\n",
    "\n",
    "## get deeplake vector database\n",
    "def get_vector_database(id, dataset_name):\n",
    "    my_activeloop_org_id = id # \"hunter\"\n",
    "    my_activeloop_dataset_name = dataset_name # \"Vietnamese-law-RAG\"\n",
    "    dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def get_index(vector_store):\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query generation / rewriting\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "query_str = \"Vượt đèn đỏ sẽ bị gì?\"\n",
    "\n",
    "prompt_for_generating_query = PromptTemplate(\n",
    "    \"\"\"Bạn là một trợ lý tuyệt vời trong việc tạo ra các câu truy vấn tìm kiếm dựa trên một câu truy vấn đầu vào. Hãy tạo ra {num_queries} truy vấn tìm kiếm liên quan đến câu truy vấn đầu vào được cung cấp dưới đây, mỗi câu trên một dòng. Hãy nhớ, trả lời bằng tiếng Việt nhé! Chỉ trả về các truy vấn đã được tạo ra.\"\n",
    "\n",
    "### Query: {query}\n",
    "\n",
    "### Queries:\"\"\"\n",
    ")\n",
    "\n",
    "def generate_queries(llm, query_str, num_queries=4):\n",
    "    fmt_prompt = prompt_for_generating_query.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries =  response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "def run_queries(queries, retrievers):\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.retrieve(query))\n",
    "    \n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, tasks)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    \n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "def get_bm25_retriever(index, similarity_top_k=13625):\n",
    "    source_nodes = index.as_retriever(similarity_top_k=similarity_top_k).retrieve(\"test\")\n",
    "    nodes = [x.node for x in source_nodes]\n",
    "    bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=3)\n",
    "    return bm25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://hunter/Vietnamese-legal-data already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = get_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "llm = get_llm()\n",
    "Settings.llm = llm\n",
    "vector_store = get_vector_database(\"hunter\", \"Vietnamese-legal-data\")\n",
    "index = get_index(vector_store=vector_store)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=3)\n",
    "bm25_retriever = get_bm25_retriever(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "replacement = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "cohere_rerank = CohereRerank(model=\"rerank-multilingual-v2.0\", api_key=os.getenv('COHERE_API_KEY'), top_n=3)  # remain top 3 relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "phogpt = Ollama(model=\"phogpt\")\n",
    "Settings.llm = phogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=25,\n",
    "    vector_store_query_mode=\"hybrid\", \n",
    "    alpha=0.5,\n",
    "    node_postprocessors = [replacement, cohere_rerank],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f61458bf340>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7f60ca924af0>, completion_to_prompt=<function default_completion_to_prompt at 0x7f60ca77dc60>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='phogpt', temperature=0.75, context_window=3900, request_timeout=30.0, prompt_key='prompt', additional_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Các bước để thành lập một công ty nhà hàng được nêu trong Bộ Luật Nhà Hàng và Thực Phẩm, Phần 7, Điều 9. Điều này bao gồm việc chỉ định người quản lý (Điều 9.2.3), đăng ký với cơ quan có thẩm quyền của quốc gia sở tại hoặc khu vực tài phán địa phương khác.\n",
      "\n",
      "Để mở một doanh nghiệp nhà hàng mới ở Việt Nam đòi hỏi phải tuân thủ các bước sau:\n",
      "\n",
      "1. Xác định mục tiêu kinh doanh và đối tượng khách hàng: Điều này giúp xác định loại hình công ty nào sẽ phù hợp nhất với hoạt động của bạn (ví dụ: công ty tư nhân hoặc tập đoàn).\n",
      "2. Đăng ký thành lập tại một quốc gia có quy định cấp phép cho nhà hàng: Điều này đảm bảo rằng tất cả các thủ tục và giấy tờ cần thiết để mở doanh nghiệp được tuân thủ.\n",
      "3. Xin giấy phép và giấy phép cần thiết từ chính phủ: Điều này bao gồm việc nộp đơn xin chứng nhận vệ sinh, chứng nhận an toàn thực phẩm (FSSC 22000) hoặc giấy phép kinh doanh có điều kiện khác cho hoạt động của bạn trong ngành nhà hàng ở một số quốc gia nhất định.\n",
      "4. Đăng ký tên công ty và địa chỉ văn phòng: Điều này đảm bảo rằng tất cả các thủ tục liên quan đến việc đăng ký được tuân thủ đúng cách (ví dụ: thuế và báo cáo).\n",
      "5. Thuê nhân viên có trình độ phù hợp cho doanh nghiệp của bạn: Điều này bao gồm tuyển dụng những người quản lý, đầu bếp hoặc phụ bếp chuyên nghiệp để giám sát hoạt động kinh doanh.\n",
      "6. Thiết lập cơ sở vật chất cần thiết phục vụ khách hàng (ví dụ: địa điểm nhà hàng) và đảm bảo rằng tất cả các giấy phép được yêu cầu đã có hiệu lực trước khi bắt đầu công việc thực tế của bạn với tư cách là chủ sở hữu hoặc người quản lý một trong những hoạt động này.\n",
      "\n",
      "Điều quan trọng là phải xem xét cẩn thận luật pháp, quy định hiện hành tại quốc gia nơi doanh nghiệp dự kiến sẽ mở và điều chỉnh chúng cho phù hợp. Điều này bao gồm các yêu cầu về vốn tối thiểu để thành lập công ty nhà hàng ở hầu hết các quốc gia hoặc khu vực tài phán.\n",
      "\n",
      "Điều quan trọng là phải tham khảo ý kiến luật sư có kinh nghiệm trong lĩnh vực cấp phép, an toàn thực phẩm (FSSA) và bất kỳ quy định liên quan nào khác.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"các bước mở nhà hàng về mặt pháp luật\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_str = \"Hình vi trộm cắp hơn 5 triệu đồng sẽ bị xử phạt như thế nào?\"\n",
    "# query_str = \"Làm sao để có thể mở nhà hàng?\"\n",
    "query_str = \"Tôi đang có nhu cầu muốn mở tiệm net để kinh doanh thì theo quy định tôi cần làm gì\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ 1 queries ban đầu, tạo ra n queries -> làm rõ nghĩa hơn cho câu query gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "retrievers = [vector_retriever, bm25_retriever]\n",
    "queries = generate_queries(Gemini(), query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever tất cả các node với 2 retrievers cho câu query gốc và các queries mới tạo\n",
    "Chỉ lấy các node có score > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for retriever in retrievers:\n",
    "    for q in queries:\n",
    "        retrieved_nodes = retriever.retrieve(q)\n",
    "        for n in retrieved_nodes:\n",
    "            if n.score > 0.7:\n",
    "                nodes.append(n)\n",
    "    nodes.append(retriever.retrieve(query_str)[0])\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng reranker để sắp xếp lại độ tương đồng cho câu query gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "replacement = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "nodes = replacement.postprocess_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(model=\"rerank-multilingual-v2.0\", api_key=os.getenv('COHERE_API_KEY'), top_n=3)  # remain top 3 relevant\n",
    "\n",
    "from llama_index.core.schema import QueryBundle\n",
    "final_nodes = cohere_rerank.postprocess_nodes(\n",
    "    nodes, QueryBundle(query_str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_= \"\"\"\n",
    "Bạn là một trợ lý ảo về tư vấn pháp luật. Nhiệm vụ của bạn là sinh ra câu trả lời dựa vào hướng dẫn được cung cấp, với tài liệu tham khảo được đánh giá độ liên quan từ cao đến thấp.\n",
    "\n",
    "Ví dụ:\n",
    "<INS>\n",
    "prompt templates\n",
    "<QUES>\n",
    "<REF>\n",
    "```\n",
    "Prompt template sẽ gồm: câu hỏi <QUES> và tài liệu tham khảo <REF>.\n",
    "\n",
    "Quy tắc trả lời:\n",
    "Chỉ dựa vào thông tin trong <REF> để trả lời, không sử dụng kiến thức sẵn có.\n",
    "Trả lời như thể đây là kiến thức của bạn, không dùng cụm từ như \"dựa vào thông tin bạn cung cấp\".\n",
    "Nếu không đủ thông tin trong <REF>, hãy nói rằng tài liệu không đủ để trả lời.\n",
    "Từ chối trả lời nếu câu hỏi chứa nội dung tiêu cực hoặc không lành mạnh.\n",
    "Trả lời tự nhiên và thoải mái như một chuyên gia.\n",
    "Định dạng câu trả lời:\n",
    "Trả lời phải tự nhiên, không chứa các từ như: prompt templates, <QUES>, <INS>, <REF>.\n",
    "Không cần lặp lại câu hỏi trong câu trả lời.\n",
    "Thông tin cung cấp: <INS>\n",
    "\n",
    "<QUES>={query_str}\n",
    "\n",
    "<REF>={context_str}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ = \"\"\"\n",
    "Bạn là một trợ lý ảo về tư vấn pháp luật. Nhiệm vụ của bạn là sinh ra câu trả lời dựa vào hướng dẫn được cung cấp, kết hợp thông tin từ tài liệu tham khảo với khả năng suy luận và kiến thức chuyên môn của bạn để đưa ra câu trả lời sâu sắc và chi tiết.\n",
    "\n",
    "Ví dụ: Nếu văn bản được truy xuất nói về một điểm pháp luật, nhưng câu hỏi liên quan đến một tình huống thực tế, bạn cần dựa vào thông tin đó để giải quyết hoặc trả lời thấu đáo câu hỏi.\n",
    "\n",
    "Prompt templates này sẽ chứa phần: câu hỏi được kí hiệu bằng thẻ <QUES> và phần tài liệu tham khảo được kí hiệu bằng thẻ <REF>.\n",
    "\n",
    "# Quy tắc trả lời:\n",
    "1. Kết hợp thông tin từ phần tài liệu tham khảo <REF> với khả năng suy luận và kiến thức chuyên môn của bạn để đưa ra câu trả lời chi tiết và sâu sắc.\n",
    "2. Trả lời như thể đây là kiến thức của bạn, không dùng các cụm từ như: \"dựa vào thông tin bạn cung cấp\", \"dựa vào thông tin dưới đây\", \"dựa vào tài liệu tham khảo\",...\n",
    "3. Nếu không đủ thông tin trong phần tài liệu tham khảo <REF> để trả lời, hãy nói rằng tài liệu không đủ để trả lời.\n",
    "4. Từ chối trả lời nếu câu hỏi chứa nội dung tiêu cực hoặc không lành mạnh.\n",
    "5. Trả lời với giọng điệu tự nhiên và thoải mái như một chuyên gia thực sự.\n",
    "\n",
    "# Định dạng câu trả lời:\n",
    "1. Câu trả lời phải tự nhiên và không chứa các từ như: prompt templates, <QUES>, <INS>, <REF>.\n",
    "2. Không cần lặp lại câu hỏi trong câu trả lời.\n",
    "\n",
    "Thông tin cung cấp: <INS>\n",
    "\n",
    "<QUES>={query_str}\n",
    "\n",
    "<REF>={context_str}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_ = PromptTemplate(prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "context = \"\\n\\n\".join([node.get_content() for node in final_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "phogpt = Ollama(model=\"phogpt\")\n",
    "Settings.llm = phogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "print(query_engine.query(\"Người đang ở tù thì có quyền thừa kế tài sản không?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "print(Gemini().complete(prompt_.format(query_str=query_str, context_str=context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phogpt.complete(prompt_.format(query_str=query_str, context_str=context)).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query_str, retrievers, llm, reranker, generator_prompt):\n",
    "    queries = generate_queries(Gemini(), query_str)\n",
    "    \n",
    "    nodes = []\n",
    "    for retriever in retrievers:\n",
    "        for q in queries:\n",
    "            retrieved_nodes = retriever.retrieve(q)\n",
    "            for n in retrieved_nodes:\n",
    "                nodes.append(n)\n",
    "        nodes.append(retriever.retrieve(query_str)[0])\n",
    "    \n",
    "    nodes = [node for node in nodes if node.score >= 0.75]\n",
    "    final_nodes = reranker.postprocess_nodes(\n",
    "        nodes, QueryBundle(query_str)\n",
    "    )\n",
    "\n",
    "    context = \"\\n\\n\".join([node.get_content() for node in final_nodes])\n",
    "    response = llm.complete(generator_prompt.format(context_str=context, query_str=query_str))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer(\"Tôi đang có nhu cầu \\\n",
    "muốn mở tiệm net để\\\n",
    "kinh doanh thì theo quy định tôi cần làm gì\", retrievers, phogpt, cohere_rerank, prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "- negenerate queries\n",
    "- retrieve all nodes for those queries => hybrid search\n",
    "- rerank \n",
    "- generate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
