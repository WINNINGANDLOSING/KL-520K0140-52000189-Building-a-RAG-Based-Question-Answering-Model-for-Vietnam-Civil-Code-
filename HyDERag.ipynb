{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding model\n",
    "def get_embedding_model(model_name=\"/teamspace/studios/this_studio/bge-small-en-v1.5\"):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    return embed_model\n",
    "\n",
    "# generator model\n",
    "def get_llm(model_name=\"llama3-8b-8192\"):\n",
    "    llm = Groq(model=model_name, api_key=os.getenv(\"GROQ_API\"), temperature=0.8)\n",
    "    return llm\n",
    "\n",
    "## get deeplake vector database\n",
    "def get_vector_database(id, dataset_name):\n",
    "    my_activeloop_org_id = id # \"hunter\"\n",
    "    my_activeloop_dataset_name = dataset_name # \"Vietnamese-law-RAG\"\n",
    "    dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def get_index(vector_store):\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query generation / rewriting\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "query_str = \"Vượt đèn đỏ sẽ bị gì?\"\n",
    "\n",
    "prompt_for_generating_query = PromptTemplate(\n",
    "\"\"\"Bạn là một trợ lý xuất sắc trong việc tạo ra các câu truy vấn tìm kiếm liên quan. Dựa trên câu truy vấn đầu vào dưới đây, hãy tạo ra {num_queries} truy vấn tìm kiếm liên quan, mỗi câu trên một dòng. Lưu ý, trả lời bằng tiếng Việt và chỉ trả về các truy vấn đã tạo ra.\n",
    "\n",
    "### Câu truy vấn đầu vào: {query}\n",
    "\n",
    "### Các câu truy vấn:\"\"\"\n",
    ")\n",
    "\n",
    "def generate_queries(llm, query_str, num_queries=4):\n",
    "    fmt_prompt = prompt_for_generating_query.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries =  response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "def run_queries(queries, retrievers):\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.retrieve(query))\n",
    "    \n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, tasks)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://hunter/Vietnamese-legal-data already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "## set embedding model\n",
    "embed_model = get_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "## set llm \n",
    "llm = Ollama(model=\"vistral\", request_timeout=120.0, max_new_tokens=2000) # get_llm()\n",
    "Settings.llm = llm\n",
    "\n",
    "## get vector store, index, and two retrievers\n",
    "vector_store = get_vector_database(\"hunter\", \"Vietnamese-legal-data\")\n",
    "index = get_index(vector_store=vector_store)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Vượt đèn đỏ bị phạt bao nhiêu tiền?\"\n",
    "response = query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
